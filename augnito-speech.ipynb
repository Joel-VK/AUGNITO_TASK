{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9772230,"sourceType":"datasetVersion","datasetId":5985780}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install Necessary Libraries\n!pip install transformers torchaudio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T12:53:08.515991Z","iopub.execute_input":"2024-10-31T12:53:08.516290Z","iopub.status.idle":"2024-10-31T12:53:21.120221Z","shell.execute_reply.started":"2024-10-31T12:53:08.516252Z","shell.execute_reply":"2024-10-31T12:53:21.119034Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchaudio) (2.4.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchaudio) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchaudio) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:54:32.372884Z","iopub.execute_input":"2024-10-31T12:54:32.373282Z","iopub.status.idle":"2024-10-31T12:54:55.479574Z","shell.execute_reply.started":"2024-10-31T12:54:32.373247Z","shell.execute_reply":"2024-10-31T12:54:55.478438Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!du -h /kaggle/working  # List file sizes in the working directory\n!rm -rf /kaggle/working/large_file_or_directory  # Replace with the path to delete\n\nimport shutil\nshutil.rmtree('/root/.cache/huggingface', ignore_errors=True)\nshutil.rmtree('/root/.cache/torch', ignore_errors=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:26:59.465286Z","iopub.execute_input":"2024-10-31T17:26:59.466123Z","iopub.status.idle":"2024-10-31T17:27:01.575876Z","shell.execute_reply.started":"2024-10-31T17:26:59.466081Z","shell.execute_reply":"2024-10-31T17:27:01.574348Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"486M\t/kaggle/working/results/checkpoint-534\n1.1G\t/kaggle/working/results/checkpoint-8960\n1.1G\t/kaggle/working/results/checkpoint-7840\n1.1G\t/kaggle/working/results/checkpoint-560\n1.1G\t/kaggle/working/results/checkpoint-5600\n1.1G\t/kaggle/working/results/checkpoint-3360\n1.1G\t/kaggle/working/results/checkpoint-1680\n1.1G\t/kaggle/working/results/checkpoint-5040\n1.1G\t/kaggle/working/results/checkpoint-6160\n1.1G\t/kaggle/working/results/checkpoint-4480\n1.1G\t/kaggle/working/results/checkpoint-9520\n1.1G\t/kaggle/working/results/checkpoint-2240\n1.1G\t/kaggle/working/results/checkpoint-2800\n1.1G\t/kaggle/working/results/checkpoint-1120\n1.1G\t/kaggle/working/results/checkpoint-3920\n1.1G\t/kaggle/working/results/checkpoint-10080\n1.1G\t/kaggle/working/results/checkpoint-6720\n1.1G\t/kaggle/working/results/checkpoint-7280\n1.1G\t/kaggle/working/results/checkpoint-8400\n20G\t/kaggle/working/results\n4.0K\t/kaggle/working/.virtual_documents\n20G\t/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 2: Import Necessary Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import Wav2Vec2FeatureExtractor, HubertForSequenceClassification, Trainer, TrainingArguments\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Step 3: Define the Data Directory and Emotion Labels\ndata_dir = '/kaggle/input/augnito-speech-task/Emotions'\nemotion_labels = ['Angry', 'Happy', 'Sad', 'Neutral', 'Fearful', 'Disgusted', 'Suprised']\n\n# Step 4: Create a DataFrame with File Paths and Labels\nfile_paths = []\nlabels = []\n\n# Load file paths and labels based on folder names\nfor emotion in emotion_labels:\n    emotion_dir = os.path.join(data_dir, emotion)\n    if not os.path.isdir(emotion_dir):\n        print(f\"Warning: Directory for emotion '{emotion}' not found.\")\n        continue\n\n    for root, _, files in os.walk(emotion_dir):\n        for file in files:\n            if file.endswith('.wav'):\n                file_path = os.path.join(root, file)\n                file_paths.append(file_path)\n                labels.append(emotion)\n\nif not file_paths:\n    raise ValueError(\"No .wav files found. Check the data directory path.\")\n\n# Confirm the lengths of file_paths and labels are the same\nif len(file_paths) != len(labels):\n    raise ValueError(\"Mismatch between file paths and labels.\")\n\n# Create DataFrame\ndf = pd.DataFrame({'file_path': file_paths, 'emotion': labels})\n\n# Encode Labels\ndf['label'] = df['emotion'].astype('category').cat.codes\nlabel2id = {label: idx for idx, label in enumerate(emotion_labels)}\nid2label = {v: k for k, v in label2id.items()}\n\n# Step 5: Split Data into Train, Validation, and Test Sets (70-15-15) with Stratification\ntrain_df, test_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1765, stratify=train_df['label'], random_state=42)\n\n# Step 6: Define a Dataset Class for Loading Data and Tokenizing\nclass EmotionDataset(Dataset):\n    def __init__(self, dataframe, feature_extractor, max_length=16000):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.feature_extractor = feature_extractor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        file_path = self.dataframe.loc[idx, 'file_path']\n        label = self.dataframe.loc[idx, 'label']\n        \n        # Load audio file and truncate or pad to max_length\n        speech_array, _ = librosa.load(file_path, sr=16000)\n        if len(speech_array) > self.max_length:\n            speech_array = speech_array[:self.max_length]\n        else:\n            pad_length = self.max_length - len(speech_array)\n            speech_array = np.pad(speech_array, (0, pad_length), 'constant')\n        \n        # Extract features and remove the extra dimension\n        inputs = self.feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\")\n        inputs['input_values'] = inputs['input_values'].squeeze()\n        \n        # Convert label to torch.long to avoid RuntimeError\n        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n        \n        return inputs\n\n# Step 7: Initialize the Feature Extractor and Model\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\nmodel = HubertForSequenceClassification.from_pretrained(\n    \"facebook/hubert-base-ls960\",\n    num_labels=len(label2id),\n    problem_type=\"single_label_classification\"\n).to(device)\n\n# Step 8: Create Datasets\ntrain_dataset = EmotionDataset(train_df, feature_extractor)\nval_dataset = EmotionDataset(val_df, feature_extractor)\ntest_dataset = EmotionDataset(test_df, feature_extractor)\n\n# Step 9: Define Training Arguments with adaptive learning and checkpoint management\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=20,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\",\n    save_total_limit=1  # Only keep the best model checkpoint\n)\n\n# Define compute_metrics function\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(labels, preds)\n    \n    # Generate classification report for precision, recall, and f1-score\n    report = classification_report(\n        labels, preds, labels=list(label2id.values()), target_names=emotion_labels, output_dict=True, zero_division=0\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'precision': report['weighted avg']['precision'],\n        'recall': report['weighted avg']['recall'],\n        'f1': report['weighted avg']['f1-score']\n    }\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n\n# Define ReduceLROnPlateau scheduler with desired parameters\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(\n    optimizer=optimizer,\n    mode='max',           # Maximize accuracy\n    factor=0.5,           # Reduce learning rate by half\n    patience=3,           # Wait 3 epochs for improvement\n    threshold=0.001,      # Minimum change to qualify as improvement\n    verbose=True          # Log LR adjustments\n)\n\n# Custom Trainer to support ReduceLROnPlateau\nclass TrainerWithReduceLROnPlateau(Trainer):\n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n        # Extract accuracy and adjust learning rate\n        if 'eval_accuracy' in metrics:\n            scheduler.step(metrics['eval_accuracy'])\n        return metrics\n\n# Initialize the Trainer with the custom Trainer class\ntrainer = TrainerWithReduceLROnPlateau(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler)  # Provide optimizer and scheduler\n)\n\n# Step 10: Train the Model\ntrainer.train()\n\n# Step 11: Evaluate on Test Data\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Test Results: {test_results}\")\n\n# Step 12: Model Predictions and Classification Report\npredictions = trainer.predict(test_dataset)\nprint(classification_report(predictions.label_ids, np.argmax(predictions.predictions, axis=1), target_names=emotion_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:42:19.163295Z","iopub.execute_input":"2024-10-31T17:42:19.163999Z","iopub.status.idle":"2024-10-31T18:42:12.779930Z","shell.execute_reply.started":"2024-10-31T17:42:19.163953Z","shell.execute_reply":"2024-10-31T18:42:12.778923Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11200/11200 58:57, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.779800</td>\n      <td>1.631117</td>\n      <td>0.344792</td>\n      <td>0.396553</td>\n      <td>0.344792</td>\n      <td>0.261679</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.589700</td>\n      <td>1.432611</td>\n      <td>0.454167</td>\n      <td>0.532440</td>\n      <td>0.454167</td>\n      <td>0.395898</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.434400</td>\n      <td>1.313417</td>\n      <td>0.504167</td>\n      <td>0.543898</td>\n      <td>0.504167</td>\n      <td>0.489897</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.334800</td>\n      <td>1.234539</td>\n      <td>0.532292</td>\n      <td>0.590122</td>\n      <td>0.532292</td>\n      <td>0.520463</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.271500</td>\n      <td>1.221803</td>\n      <td>0.541667</td>\n      <td>0.616314</td>\n      <td>0.541667</td>\n      <td>0.525393</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.189300</td>\n      <td>1.133391</td>\n      <td>0.558333</td>\n      <td>0.596985</td>\n      <td>0.558333</td>\n      <td>0.562612</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.154300</td>\n      <td>1.123258</td>\n      <td>0.567708</td>\n      <td>0.616985</td>\n      <td>0.567708</td>\n      <td>0.556111</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.100300</td>\n      <td>1.115040</td>\n      <td>0.576042</td>\n      <td>0.630896</td>\n      <td>0.576042</td>\n      <td>0.565356</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.037200</td>\n      <td>1.069845</td>\n      <td>0.592187</td>\n      <td>0.623005</td>\n      <td>0.592187</td>\n      <td>0.593442</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.002100</td>\n      <td>1.140706</td>\n      <td>0.585938</td>\n      <td>0.620855</td>\n      <td>0.585938</td>\n      <td>0.579552</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.961300</td>\n      <td>1.072180</td>\n      <td>0.594792</td>\n      <td>0.631482</td>\n      <td>0.594792</td>\n      <td>0.593370</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.936500</td>\n      <td>1.108109</td>\n      <td>0.596875</td>\n      <td>0.634863</td>\n      <td>0.596875</td>\n      <td>0.593931</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.927300</td>\n      <td>1.173622</td>\n      <td>0.583854</td>\n      <td>0.621892</td>\n      <td>0.583854</td>\n      <td>0.581679</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.905600</td>\n      <td>1.095390</td>\n      <td>0.616146</td>\n      <td>0.631909</td>\n      <td>0.616146</td>\n      <td>0.610586</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.884000</td>\n      <td>1.084689</td>\n      <td>0.610938</td>\n      <td>0.631872</td>\n      <td>0.610938</td>\n      <td>0.608394</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.857000</td>\n      <td>1.082749</td>\n      <td>0.617188</td>\n      <td>0.635527</td>\n      <td>0.617188</td>\n      <td>0.615031</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.834600</td>\n      <td>1.160384</td>\n      <td>0.608333</td>\n      <td>0.637229</td>\n      <td>0.608333</td>\n      <td>0.606231</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.788800</td>\n      <td>1.133961</td>\n      <td>0.619271</td>\n      <td>0.664425</td>\n      <td>0.619271</td>\n      <td>0.619822</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.799600</td>\n      <td>1.097331</td>\n      <td>0.622917</td>\n      <td>0.635825</td>\n      <td>0.622917</td>\n      <td>0.623239</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.775000</td>\n      <td>1.208839</td>\n      <td>0.605208</td>\n      <td>0.658308</td>\n      <td>0.605208</td>\n      <td>0.603632</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test Results: {'eval_loss': 1.0623122453689575, 'eval_accuracy': 0.6307291666666667, 'eval_precision': 0.642927336852053, 'eval_recall': 0.6307291666666667, 'eval_f1': 0.6305208988662325, 'eval_runtime': 32.4509, 'eval_samples_per_second': 59.166, 'eval_steps_per_second': 3.698, 'epoch': 20.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Angry       0.81      0.72      0.76       325\n       Happy       0.57      0.57      0.57       280\n         Sad       0.66      0.48      0.55       307\n     Neutral       0.57      0.64      0.61       325\n     Fearful       0.61      0.79      0.68       269\n   Disgusted       0.55      0.61      0.58       325\n    Suprised       0.90      0.62      0.73        89\n\n    accuracy                           0.63      1920\n   macro avg       0.67      0.63      0.64      1920\nweighted avg       0.64      0.63      0.63      1920\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 1: Import Necessary Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import Wav2Vec2FeatureExtractor, WavLMModel\nfrom torch import nn, optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Step 2: Define the Data Directory and Emotion Labels\ndata_dir = '/kaggle/input/augnito-speech-task/Emotions'\nemotion_labels = ['Angry', 'Happy', 'Sad', 'Neutral', 'Fearful', 'Disgusted', 'Suprised']\n\n# Step 3: Create a DataFrame with File Paths and Labels\nfile_paths, labels = [], []\nfor emotion in emotion_labels:\n    emotion_dir = os.path.join(data_dir, emotion)\n    if not os.path.isdir(emotion_dir):\n        print(f\"Warning: Directory for emotion '{emotion}' not found.\")\n        continue\n    for root, _, files in os.walk(emotion_dir):\n        for file in files:\n            if file.endswith('.wav'):\n                file_path = os.path.join(root, file)\n                file_paths.append(file_path)\n                labels.append(emotion)\n\nif not file_paths:\n    raise ValueError(\"No .wav files found. Check the data directory path.\")\n\n# Create DataFrame\ndf = pd.DataFrame({'file_path': file_paths, 'emotion': labels})\ndf['label'] = df['emotion'].astype('category').cat.codes\nlabel2id = {label: idx for idx, label in enumerate(emotion_labels)}\nid2label = {v: k for k, v in label2id.items()}\n\n# Step 4: Extract Embeddings Using WavLM Base\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base\")\nwavlm_model = WavLMModel.from_pretrained(\"microsoft/wavlm-base\").to(device)\nwavlm_model.eval()\n\ndef extract_embedding(file_path, feature_extractor, model):\n    speech_array, _ = librosa.load(file_path, sr=16000)\n    inputs = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Mean pooling\n    return embedding\n\n# Extract embeddings\nembeddings, labels = [], []\nfor _, row in df.iterrows():\n    file_path, label = row['file_path'], row['label']\n    embedding = extract_embedding(file_path, feature_extractor, wavlm_model)\n    embeddings.append(embedding)\n    labels.append(label)\n\n# Convert embeddings and labels into a DataFrame\nembedding_df = pd.DataFrame(np.concatenate(embeddings, axis=0))\nembedding_df['label'] = labels\nembedding_df.to_csv(\"precomputed_embeddings.csv\", index=False)\n\n# Step 5: Train-Test Split\nX = embedding_df.drop(columns=[\"label\"]).values  # Features (embeddings)\ny = embedding_df[\"label\"].values  # Labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\n# Step 6: Define a Complex Classifier Model\nclass EnhancedClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(EnhancedClassifier, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\ninput_dim = X_train.shape[1]\nnum_classes = len(np.unique(y))\nmodel = EnhancedClassifier(input_dim, num_classes).to(device)\n\n# Step 7: Training the Classifier with Scheduler and Best Model Saving\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-4)  # Initial learning rate\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)  # Reduce on val loss increase\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n\nbest_val_loss = float('inf')\nbest_model_path = \"best_model.pth\"\n\n# Training Loop\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n\n    # Validation phase\n    model.eval()\n    with torch.no_grad():\n        val_outputs = model(X_test_tensor)\n        val_loss = criterion(val_outputs, y_test_tensor).item()\n        _, val_predicted = torch.max(val_outputs, 1)\n        val_accuracy = accuracy_score(y_test_tensor.cpu(), val_predicted.cpu())\n\n    # Step the scheduler with validation loss\n    scheduler.step(val_loss)\n\n    # Check if current model is the best and save it\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)  # Overwrite best model\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n# Load the best model for final evaluation\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Step 8: Final Evaluation on Test Set\nmodel.eval()\nwith torch.no_grad():\n    test_outputs = model(X_test_tensor)\n    _, predicted = torch.max(test_outputs, 1)\n    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(classification_report(y_test_tensor.cpu(), predicted.cpu(), target_names=emotion_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T02:09:02.351909Z","iopub.execute_input":"2024-11-01T02:09:02.352260Z","iopub.status.idle":"2024-11-01T02:14:18.913790Z","shell.execute_reply.started":"2024-11-01T02:09:02.352226Z","shell.execute_reply":"2024-11-01T02:14:18.912799Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/200], Loss: 2.0145, Val Loss: 1.9380, Val Accuracy: 0.1714\nEpoch [2/200], Loss: 1.8145, Val Loss: 1.9320, Val Accuracy: 0.1776\nEpoch [3/200], Loss: 1.6897, Val Loss: 1.9250, Val Accuracy: 0.1906\nEpoch [4/200], Loss: 1.5845, Val Loss: 1.9167, Val Accuracy: 0.2120\nEpoch [5/200], Loss: 1.5001, Val Loss: 1.9071, Val Accuracy: 0.2479\nEpoch [6/200], Loss: 1.4190, Val Loss: 1.8957, Val Accuracy: 0.2969\nEpoch [7/200], Loss: 1.3485, Val Loss: 1.8821, Val Accuracy: 0.3125\nEpoch [8/200], Loss: 1.2854, Val Loss: 1.8663, Val Accuracy: 0.3333\nEpoch [9/200], Loss: 1.2186, Val Loss: 1.8484, Val Accuracy: 0.3474\nEpoch [10/200], Loss: 1.1632, Val Loss: 1.8280, Val Accuracy: 0.3589\nEpoch [11/200], Loss: 1.1219, Val Loss: 1.8047, Val Accuracy: 0.3766\nEpoch [12/200], Loss: 1.0755, Val Loss: 1.7784, Val Accuracy: 0.3922\nEpoch [13/200], Loss: 1.0409, Val Loss: 1.7488, Val Accuracy: 0.4099\nEpoch [14/200], Loss: 1.0013, Val Loss: 1.7160, Val Accuracy: 0.4266\nEpoch [15/200], Loss: 0.9746, Val Loss: 1.6806, Val Accuracy: 0.4474\nEpoch [16/200], Loss: 0.9485, Val Loss: 1.6432, Val Accuracy: 0.4719\nEpoch [17/200], Loss: 0.9193, Val Loss: 1.6044, Val Accuracy: 0.4974\nEpoch [18/200], Loss: 0.9012, Val Loss: 1.5643, Val Accuracy: 0.5141\nEpoch [19/200], Loss: 0.8765, Val Loss: 1.5231, Val Accuracy: 0.5286\nEpoch [20/200], Loss: 0.8559, Val Loss: 1.4820, Val Accuracy: 0.5474\nEpoch [21/200], Loss: 0.8371, Val Loss: 1.4427, Val Accuracy: 0.5583\nEpoch [22/200], Loss: 0.8213, Val Loss: 1.4048, Val Accuracy: 0.5646\nEpoch [23/200], Loss: 0.8057, Val Loss: 1.3681, Val Accuracy: 0.5698\nEpoch [24/200], Loss: 0.7816, Val Loss: 1.3306, Val Accuracy: 0.5797\nEpoch [25/200], Loss: 0.7686, Val Loss: 1.2917, Val Accuracy: 0.5865\nEpoch [26/200], Loss: 0.7587, Val Loss: 1.2527, Val Accuracy: 0.5984\nEpoch [27/200], Loss: 0.7413, Val Loss: 1.2163, Val Accuracy: 0.6068\nEpoch [28/200], Loss: 0.7232, Val Loss: 1.1809, Val Accuracy: 0.6109\nEpoch [29/200], Loss: 0.7116, Val Loss: 1.1470, Val Accuracy: 0.6177\nEpoch [30/200], Loss: 0.7003, Val Loss: 1.1120, Val Accuracy: 0.6266\nEpoch [31/200], Loss: 0.6923, Val Loss: 1.0758, Val Accuracy: 0.6438\nEpoch [32/200], Loss: 0.6781, Val Loss: 1.0409, Val Accuracy: 0.6578\nEpoch [33/200], Loss: 0.6696, Val Loss: 1.0131, Val Accuracy: 0.6578\nEpoch [34/200], Loss: 0.6501, Val Loss: 0.9875, Val Accuracy: 0.6594\nEpoch [35/200], Loss: 0.6476, Val Loss: 0.9595, Val Accuracy: 0.6714\nEpoch [36/200], Loss: 0.6363, Val Loss: 0.9281, Val Accuracy: 0.6833\nEpoch [37/200], Loss: 0.6268, Val Loss: 0.9002, Val Accuracy: 0.6953\nEpoch [38/200], Loss: 0.6189, Val Loss: 0.8769, Val Accuracy: 0.7042\nEpoch [39/200], Loss: 0.6013, Val Loss: 0.8523, Val Accuracy: 0.7135\nEpoch [40/200], Loss: 0.5961, Val Loss: 0.8272, Val Accuracy: 0.7229\nEpoch [41/200], Loss: 0.5940, Val Loss: 0.8050, Val Accuracy: 0.7276\nEpoch [42/200], Loss: 0.5797, Val Loss: 0.7846, Val Accuracy: 0.7391\nEpoch [43/200], Loss: 0.5698, Val Loss: 0.7629, Val Accuracy: 0.7427\nEpoch [44/200], Loss: 0.5613, Val Loss: 0.7434, Val Accuracy: 0.7443\nEpoch [45/200], Loss: 0.5550, Val Loss: 0.7251, Val Accuracy: 0.7562\nEpoch [46/200], Loss: 0.5467, Val Loss: 0.7123, Val Accuracy: 0.7573\nEpoch [47/200], Loss: 0.5385, Val Loss: 0.6958, Val Accuracy: 0.7630\nEpoch [48/200], Loss: 0.5266, Val Loss: 0.6830, Val Accuracy: 0.7625\nEpoch [49/200], Loss: 0.5220, Val Loss: 0.6729, Val Accuracy: 0.7677\nEpoch [50/200], Loss: 0.5192, Val Loss: 0.6688, Val Accuracy: 0.7625\nEpoch [51/200], Loss: 0.5121, Val Loss: 0.6586, Val Accuracy: 0.7615\nEpoch [52/200], Loss: 0.5056, Val Loss: 0.6501, Val Accuracy: 0.7661\nEpoch [53/200], Loss: 0.4986, Val Loss: 0.6473, Val Accuracy: 0.7651\nEpoch [54/200], Loss: 0.4901, Val Loss: 0.6405, Val Accuracy: 0.7719\nEpoch [55/200], Loss: 0.4920, Val Loss: 0.6357, Val Accuracy: 0.7734\nEpoch [56/200], Loss: 0.4698, Val Loss: 0.6365, Val Accuracy: 0.7693\nEpoch [57/200], Loss: 0.4684, Val Loss: 0.6261, Val Accuracy: 0.7729\nEpoch [58/200], Loss: 0.4599, Val Loss: 0.6239, Val Accuracy: 0.7698\nEpoch [59/200], Loss: 0.4620, Val Loss: 0.6338, Val Accuracy: 0.7630\nEpoch [60/200], Loss: 0.4532, Val Loss: 0.6180, Val Accuracy: 0.7698\nEpoch [61/200], Loss: 0.4446, Val Loss: 0.6173, Val Accuracy: 0.7740\nEpoch [62/200], Loss: 0.4450, Val Loss: 0.6382, Val Accuracy: 0.7630\nEpoch [63/200], Loss: 0.4374, Val Loss: 0.6169, Val Accuracy: 0.7714\nEpoch [64/200], Loss: 0.4268, Val Loss: 0.6147, Val Accuracy: 0.7719\nEpoch [65/200], Loss: 0.4229, Val Loss: 0.6318, Val Accuracy: 0.7677\nEpoch [66/200], Loss: 0.4159, Val Loss: 0.6149, Val Accuracy: 0.7771\nEpoch [67/200], Loss: 0.4028, Val Loss: 0.6047, Val Accuracy: 0.7745\nEpoch [68/200], Loss: 0.4100, Val Loss: 0.6045, Val Accuracy: 0.7740\nEpoch [69/200], Loss: 0.4089, Val Loss: 0.6011, Val Accuracy: 0.7740\nEpoch [70/200], Loss: 0.3974, Val Loss: 0.5930, Val Accuracy: 0.7802\nEpoch [71/200], Loss: 0.3988, Val Loss: 0.5872, Val Accuracy: 0.7823\nEpoch [72/200], Loss: 0.3889, Val Loss: 0.5828, Val Accuracy: 0.7828\nEpoch [73/200], Loss: 0.3905, Val Loss: 0.5845, Val Accuracy: 0.7807\nEpoch [74/200], Loss: 0.3852, Val Loss: 0.5817, Val Accuracy: 0.7849\nEpoch [75/200], Loss: 0.3819, Val Loss: 0.5760, Val Accuracy: 0.7911\nEpoch [76/200], Loss: 0.3754, Val Loss: 0.5732, Val Accuracy: 0.7854\nEpoch [77/200], Loss: 0.3779, Val Loss: 0.5742, Val Accuracy: 0.7844\nEpoch [78/200], Loss: 0.3707, Val Loss: 0.5719, Val Accuracy: 0.7901\nEpoch [79/200], Loss: 0.3722, Val Loss: 0.5687, Val Accuracy: 0.7922\nEpoch [80/200], Loss: 0.3645, Val Loss: 0.5688, Val Accuracy: 0.7922\nEpoch [81/200], Loss: 0.3658, Val Loss: 0.5693, Val Accuracy: 0.7896\nEpoch [82/200], Loss: 0.3600, Val Loss: 0.5668, Val Accuracy: 0.7906\nEpoch [83/200], Loss: 0.3604, Val Loss: 0.5634, Val Accuracy: 0.7974\nEpoch [84/200], Loss: 0.3586, Val Loss: 0.5620, Val Accuracy: 0.8000\nEpoch [85/200], Loss: 0.3596, Val Loss: 0.5628, Val Accuracy: 0.7995\nEpoch [86/200], Loss: 0.3604, Val Loss: 0.5650, Val Accuracy: 0.7969\nEpoch [87/200], Loss: 0.3485, Val Loss: 0.5648, Val Accuracy: 0.7969\nEpoch [88/200], Loss: 0.3518, Val Loss: 0.5638, Val Accuracy: 0.8000\nEpoch [89/200], Loss: 0.3507, Val Loss: 0.5625, Val Accuracy: 0.8016\nEpoch [90/200], Loss: 0.3455, Val Loss: 0.5615, Val Accuracy: 0.8047\nEpoch [91/200], Loss: 0.3474, Val Loss: 0.5609, Val Accuracy: 0.8057\nEpoch [92/200], Loss: 0.3536, Val Loss: 0.5607, Val Accuracy: 0.8063\nEpoch [93/200], Loss: 0.3465, Val Loss: 0.5607, Val Accuracy: 0.8057\nEpoch [94/200], Loss: 0.3480, Val Loss: 0.5608, Val Accuracy: 0.8073\nEpoch [95/200], Loss: 0.3489, Val Loss: 0.5608, Val Accuracy: 0.8073\nEpoch [96/200], Loss: 0.3514, Val Loss: 0.5608, Val Accuracy: 0.8073\nEpoch [97/200], Loss: 0.3461, Val Loss: 0.5607, Val Accuracy: 0.8073\nEpoch [98/200], Loss: 0.3534, Val Loss: 0.5607, Val Accuracy: 0.8068\nEpoch [99/200], Loss: 0.3448, Val Loss: 0.5607, Val Accuracy: 0.8063\nEpoch [100/200], Loss: 0.3421, Val Loss: 0.5607, Val Accuracy: 0.8068\nEpoch [101/200], Loss: 0.3418, Val Loss: 0.5607, Val Accuracy: 0.8068\nEpoch [102/200], Loss: 0.3408, Val Loss: 0.5607, Val Accuracy: 0.8078\nEpoch [103/200], Loss: 0.3487, Val Loss: 0.5607, Val Accuracy: 0.8068\nEpoch [104/200], Loss: 0.3457, Val Loss: 0.5608, Val Accuracy: 0.8068\nEpoch [105/200], Loss: 0.3462, Val Loss: 0.5608, Val Accuracy: 0.8063\nEpoch [106/200], Loss: 0.3395, Val Loss: 0.5608, Val Accuracy: 0.8057\nEpoch [107/200], Loss: 0.3426, Val Loss: 0.5609, Val Accuracy: 0.8057\nEpoch [108/200], Loss: 0.3442, Val Loss: 0.5609, Val Accuracy: 0.8057\nEpoch [109/200], Loss: 0.3450, Val Loss: 0.5610, Val Accuracy: 0.8042\nEpoch [110/200], Loss: 0.3485, Val Loss: 0.5610, Val Accuracy: 0.8031\nEpoch [111/200], Loss: 0.3417, Val Loss: 0.5611, Val Accuracy: 0.8036\nEpoch [112/200], Loss: 0.3417, Val Loss: 0.5611, Val Accuracy: 0.8031\nEpoch [113/200], Loss: 0.3498, Val Loss: 0.5612, Val Accuracy: 0.8036\nEpoch [114/200], Loss: 0.3453, Val Loss: 0.5613, Val Accuracy: 0.8042\nEpoch [115/200], Loss: 0.3477, Val Loss: 0.5613, Val Accuracy: 0.8047\nEpoch [116/200], Loss: 0.3416, Val Loss: 0.5614, Val Accuracy: 0.8047\nEpoch [117/200], Loss: 0.3490, Val Loss: 0.5614, Val Accuracy: 0.8042\nEpoch [118/200], Loss: 0.3482, Val Loss: 0.5614, Val Accuracy: 0.8042\nEpoch [119/200], Loss: 0.3431, Val Loss: 0.5614, Val Accuracy: 0.8042\nEpoch [120/200], Loss: 0.3461, Val Loss: 0.5614, Val Accuracy: 0.8052\nEpoch [121/200], Loss: 0.3442, Val Loss: 0.5615, Val Accuracy: 0.8052\nEpoch [122/200], Loss: 0.3349, Val Loss: 0.5615, Val Accuracy: 0.8052\nEpoch [123/200], Loss: 0.3432, Val Loss: 0.5616, Val Accuracy: 0.8052\nEpoch [124/200], Loss: 0.3385, Val Loss: 0.5616, Val Accuracy: 0.8047\nEpoch [125/200], Loss: 0.3411, Val Loss: 0.5616, Val Accuracy: 0.8047\nEpoch [126/200], Loss: 0.3396, Val Loss: 0.5616, Val Accuracy: 0.8047\nEpoch [127/200], Loss: 0.3415, Val Loss: 0.5616, Val Accuracy: 0.8042\nEpoch [128/200], Loss: 0.3466, Val Loss: 0.5616, Val Accuracy: 0.8042\nEpoch [129/200], Loss: 0.3469, Val Loss: 0.5616, Val Accuracy: 0.8047\nEpoch [130/200], Loss: 0.3418, Val Loss: 0.5616, Val Accuracy: 0.8047\nEpoch [131/200], Loss: 0.3490, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [132/200], Loss: 0.3469, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [133/200], Loss: 0.3452, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [134/200], Loss: 0.3462, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [135/200], Loss: 0.3450, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [136/200], Loss: 0.3415, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [137/200], Loss: 0.3460, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [138/200], Loss: 0.3479, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [139/200], Loss: 0.3470, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [140/200], Loss: 0.3449, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [141/200], Loss: 0.3386, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [142/200], Loss: 0.3420, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [143/200], Loss: 0.3452, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [144/200], Loss: 0.3465, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [145/200], Loss: 0.3482, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [146/200], Loss: 0.3405, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [147/200], Loss: 0.3501, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [148/200], Loss: 0.3441, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [149/200], Loss: 0.3425, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [150/200], Loss: 0.3447, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [151/200], Loss: 0.3469, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [152/200], Loss: 0.3469, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [153/200], Loss: 0.3464, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [154/200], Loss: 0.3448, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [155/200], Loss: 0.3405, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [156/200], Loss: 0.3491, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [157/200], Loss: 0.3435, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [158/200], Loss: 0.3438, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [159/200], Loss: 0.3506, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [160/200], Loss: 0.3458, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [161/200], Loss: 0.3442, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [162/200], Loss: 0.3460, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [163/200], Loss: 0.3487, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [164/200], Loss: 0.3457, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [165/200], Loss: 0.3474, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [166/200], Loss: 0.3368, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [167/200], Loss: 0.3429, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [168/200], Loss: 0.3392, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [169/200], Loss: 0.3469, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [170/200], Loss: 0.3391, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [171/200], Loss: 0.3465, Val Loss: 0.5618, Val Accuracy: 0.8042\nEpoch [172/200], Loss: 0.3441, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [173/200], Loss: 0.3526, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [174/200], Loss: 0.3411, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [175/200], Loss: 0.3512, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [176/200], Loss: 0.3455, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [177/200], Loss: 0.3423, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [178/200], Loss: 0.3467, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [179/200], Loss: 0.3413, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [180/200], Loss: 0.3501, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [181/200], Loss: 0.3454, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [182/200], Loss: 0.3445, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [183/200], Loss: 0.3467, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [184/200], Loss: 0.3454, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [185/200], Loss: 0.3427, Val Loss: 0.5618, Val Accuracy: 0.8047\nEpoch [186/200], Loss: 0.3410, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [187/200], Loss: 0.3434, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [188/200], Loss: 0.3467, Val Loss: 0.5618, Val Accuracy: 0.8052\nEpoch [189/200], Loss: 0.3517, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [190/200], Loss: 0.3389, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [191/200], Loss: 0.3439, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [192/200], Loss: 0.3489, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [193/200], Loss: 0.3502, Val Loss: 0.5617, Val Accuracy: 0.8052\nEpoch [194/200], Loss: 0.3419, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [195/200], Loss: 0.3473, Val Loss: 0.5617, Val Accuracy: 0.8047\nEpoch [196/200], Loss: 0.3480, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [197/200], Loss: 0.3489, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [198/200], Loss: 0.3501, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [199/200], Loss: 0.3477, Val Loss: 0.5617, Val Accuracy: 0.8042\nEpoch [200/200], Loss: 0.3470, Val Loss: 0.5617, Val Accuracy: 0.8047\nTest Accuracy: 0.8068\n              precision    recall  f1-score   support\n\n       Angry       0.89      0.90      0.90       329\n       Happy       0.76      0.76      0.76       258\n         Sad       0.78      0.68      0.73       315\n     Neutral       0.83      0.80      0.82       348\n     Fearful       0.83      0.88      0.85       244\n   Disgusted       0.73      0.79      0.76       344\n    Suprised       0.88      0.93      0.90        82\n\n    accuracy                           0.81      1920\n   macro avg       0.81      0.82      0.82      1920\nweighted avg       0.81      0.81      0.81      1920\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/22187830.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall torch torchvision torchaudio\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T11:56:16.226665Z","iopub.execute_input":"2024-11-01T11:56:16.227557Z","iopub.status.idle":"2024-11-01T11:59:05.924215Z","shell.execute_reply.started":"2024-11-01T11:56:16.227505Z","shell.execute_reply":"2024-11-01T11:59:05.922998Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torch\n  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting torchvision\n  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio\n  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\nCollecting filelock (from torch)\n  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting typing-extensions>=4.8.0 (from torch)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting networkx (from torch)\n  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\nCollecting jinja2 (from torch)\n  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting fsspec (from torch)\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.1.0 (from torch)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nCollecting sympy==1.13.1 (from torch)\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting numpy (from torchvision)\n  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\nCollecting MarkupSafe>=2.0 (from jinja2->torch)\n  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\nDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n  Attempting uninstall: mpmath\n    Found existing installation: mpmath 1.3.0\n    Uninstalling mpmath-1.3.0:\n      Successfully uninstalled mpmath-1.3.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Uninstalling typing_extensions-4.12.2:\n      Successfully uninstalled typing_extensions-4.12.2\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.3\n    Uninstalling sympy-1.13.3:\n      Successfully uninstalled sympy-1.13.3\n  Attempting uninstall: pillow\n    Found existing installation: pillow 10.3.0\n    Uninstalling pillow-10.3.0:\n      Successfully uninstalled pillow-10.3.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.3\n    Uninstalling networkx-3.3:\n      Successfully uninstalled networkx-3.3\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.1.5\n    Uninstalling MarkupSafe-2.1.5:\n      Successfully uninstalled MarkupSafe-2.1.5\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.6.1\n    Uninstalling fsspec-2024.6.1:\n      Successfully uninstalled fsspec-2024.6.1\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.15.1\n    Uninstalling filelock-3.15.1:\n      Successfully uninstalled filelock-3.15.1\n  Attempting uninstall: jinja2\n    Found existing installation: Jinja2 3.1.4\n    Uninstalling Jinja2-3.1.4:\n      Successfully uninstalled Jinja2-3.1.4\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.4.0\n    Uninstalling torchaudio-2.4.0:\n      Successfully uninstalled torchaudio-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.1.2 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncatboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.1.2 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ncudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\ndask-cuda 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\ndask-cudf 24.8.3 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\ndatasets 3.0.1 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nfastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\ngcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.10.0 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\nibis-framework 7.1.0 requires numpy<2,>=1, but you have numpy 2.1.2 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmatplotlib 3.7.5 requires numpy<2,>=1.20, but you have numpy 2.1.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\npylibraft 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\nraft-dask 24.8.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\nrmm 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\ns3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.10.0 which is incompatible.\ntensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.2 which is incompatible.\ntensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.1.2 which is incompatible.\nthinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\nucx-py 0.39.2 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\nucxx 0.39.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\nxarray 2024.9.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-10.4.0 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 triton-3.1.0 typing-extensions-4.12.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom torch import nn, optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Define the Data Directory and Emotion Labels\ndata_dir = '/kaggle/input/augnito-speech-task/Emotions'\nemotion_labels = ['Angry', 'Happy', 'Sad', 'Neutral', 'Fearful', 'Disgusted', 'Suprised']\n\n# Create DataFrame with File Paths and Labels\nfile_paths, labels = [], []\nfor emotion in emotion_labels:\n    emotion_dir = os.path.join(data_dir, emotion)\n    for root, _, files in os.walk(emotion_dir):\n        for file in files:\n            if file.endswith('.wav'):\n                file_paths.append(os.path.join(root, file))\n                labels.append(emotion)\n\nif not file_paths:\n    raise ValueError(\"No .wav files found. Check the data directory path.\")\n\n# Check total samples loaded\nprint(f'Total samples loaded: {len(file_paths)}')\n\n# Create DataFrame\ndf = pd.DataFrame({'file_path': file_paths, 'emotion': labels})\ndf['label'] = df['emotion'].astype('category').cat.codes\nlabel2id = {label: idx for idx, label in enumerate(emotion_labels)}\nid2label = {v: k for k, v in label2id.items()}\n\n# Convert audio to mel spectrogram\ndef audio_to_mel_spectrogram(file_path, n_mels=128):\n    audio, sr = librosa.load(file_path, sr=16000)\n    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n    return log_mel_spectrogram\n\n# Preprocess all audio files and save as tensors\nspectrograms, labels = [], []\nfor _, row in df.iterrows():\n    spectrogram = audio_to_mel_spectrogram(row['file_path'])\n    spectrograms.append(spectrogram)\n    labels.append(row['label'])\n\n# Check total samples after conversion to spectrograms\nprint(f'Samples after conversion to spectrograms: {len(spectrograms)}')\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(spectrograms, labels, test_size=0.15, random_state=42)\n\n# Check training and testing samples\nprint(f'Training samples: {len(X_train)}, Testing samples: {len(X_test)}')\n\n# PyTorch Dataset\nclass AudioDataset(torch.utils.data.Dataset):\n    def __init__(self, spectrograms, labels, transform=None):\n        self.spectrograms = spectrograms\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.spectrograms)\n\n    def __getitem__(self, idx):\n        spectrogram = self.spectrograms[idx]\n        spectrogram = np.stack([spectrogram]*3, axis=0)  # Convert to 3 channels\n        spectrogram = torch.tensor(spectrogram).float()\n        if self.transform:\n            spectrogram = self.transform(spectrogram)\n        label = torch.tensor(self.labels[idx])\n        return spectrogram, label\n\n# Transformations for ResNet input\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to 224x224\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Create PyTorch DataLoader\ntrain_data = AudioDataset(X_train, y_train, transform=transform)\ntest_data = AudioDataset(X_test, y_test, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n\n# Check DataLoader batch counts\nprint(f'Train batches: {len(train_loader)}, Test batches: {len(test_loader)}')\n\n# Load ResNet18 model and freeze initial layers\nmodel = models.resnet18(pretrained=True)\n\n# Freeze initial layers\nfor name, param in model.named_parameters():\n    if \"layer3\" not in name and \"layer4\" not in name:  # Freezing up to layer2, tuning layers 3 and 4\n        param.requires_grad = False\n\n# Modify the final fully connected layer for 7-class classification\nmodel.fc = nn.Linear(model.fc.in_features, len(emotion_labels))\nmodel = model.to(device)\n\n# Training the Classifier with Cosine Annealing Scheduler and Weight Decay\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n\nbest_val_accuracy = 0\nbest_model_path = \"best_model.pth\"\n\n# Training Loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    scheduler.step()\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_accuracy = correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, \"\n          f\"Val Loss: {val_loss/len(test_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), best_model_path)\n\n# Final Evaluation on Test Set\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\nall_labels = []\nall_preds = []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(classification_report(all_labels, all_preds, target_names=emotion_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T14:22:09.278036Z","iopub.execute_input":"2024-11-01T14:22:09.278454Z","iopub.status.idle":"2024-11-01T14:33:15.212225Z","shell.execute_reply.started":"2024-11-01T14:22:09.278414Z","shell.execute_reply":"2024-11-01T14:33:15.211231Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using device: cuda\nTotal samples loaded: 12798\nSamples after conversion to spectrograms: 12798\nTraining samples: 10878, Testing samples: 1920\nTrain batches: 340, Test batches: 60\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20], Train Loss: 1.0501, Val Loss: 0.8348, Val Accuracy: 0.6854\nEpoch [2/20], Train Loss: 0.5680, Val Loss: 0.7786, Val Accuracy: 0.7078\nEpoch [3/20], Train Loss: 0.2118, Val Loss: 0.8559, Val Accuracy: 0.7109\nEpoch [4/20], Train Loss: 0.0611, Val Loss: 0.9690, Val Accuracy: 0.7130\nEpoch [5/20], Train Loss: 0.0266, Val Loss: 1.0628, Val Accuracy: 0.7094\nEpoch [6/20], Train Loss: 0.0157, Val Loss: 0.9976, Val Accuracy: 0.7229\nEpoch [7/20], Train Loss: 0.0074, Val Loss: 0.9935, Val Accuracy: 0.7182\nEpoch [8/20], Train Loss: 0.0057, Val Loss: 0.9922, Val Accuracy: 0.7245\nEpoch [9/20], Train Loss: 0.0045, Val Loss: 1.0047, Val Accuracy: 0.7224\nEpoch [10/20], Train Loss: 0.0039, Val Loss: 0.9978, Val Accuracy: 0.7219\nEpoch [11/20], Train Loss: 0.0040, Val Loss: 1.0101, Val Accuracy: 0.7271\nEpoch [12/20], Train Loss: 0.0045, Val Loss: 1.0210, Val Accuracy: 0.7214\nEpoch [13/20], Train Loss: 0.0039, Val Loss: 1.0166, Val Accuracy: 0.7240\nEpoch [14/20], Train Loss: 0.0045, Val Loss: 1.0312, Val Accuracy: 0.7276\nEpoch [15/20], Train Loss: 0.0071, Val Loss: 1.2150, Val Accuracy: 0.7094\nEpoch [16/20], Train Loss: 0.0836, Val Loss: 1.2234, Val Accuracy: 0.6932\nEpoch [17/20], Train Loss: 0.0583, Val Loss: 1.2702, Val Accuracy: 0.6818\nEpoch [18/20], Train Loss: 0.0382, Val Loss: 1.2910, Val Accuracy: 0.7036\nEpoch [19/20], Train Loss: 0.0607, Val Loss: 1.4593, Val Accuracy: 0.6823\nEpoch [20/20], Train Loss: 0.0915, Val Loss: 1.3046, Val Accuracy: 0.6896\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2597078448.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.7276\n              precision    recall  f1-score   support\n\n       Angry       0.83      0.84      0.84       329\n       Happy       0.68      0.63      0.66       258\n         Sad       0.63      0.62      0.63       315\n     Neutral       0.79      0.71      0.75       348\n     Fearful       0.75      0.75      0.75       244\n   Disgusted       0.64      0.73      0.68       344\n    Suprised       0.91      0.98      0.94        82\n\n    accuracy                           0.73      1920\n   macro avg       0.75      0.75      0.75      1920\nweighted avg       0.73      0.73      0.73      1920\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}